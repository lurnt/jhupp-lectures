{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark: Cloud Programming with Memory\n",
    "\n",
    "Lecture derived from: Zaharia et al. Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing. USENIX NSDI, 2012.\n",
    "\n",
    "* Map/Reduce style programming\n",
    "  * Data-parallel, batch, restrictive model, functional\n",
    "  * Abstractions to leverage distributed memory\n",
    "* New interfaces to in-memory computations\n",
    "  * Fault-tolerant\n",
    "  * Lazy materialization (pipelined evaluation)\n",
    "* Good support for iterative computations on in-memory data sets leads to good performance\n",
    "  * 20x over Map/Reduce\n",
    "  * No writing data to file system, loading data from file system, during iteration\n",
    "  \n",
    "### RDD: Resilient Distributed Dataset\n",
    "\n",
    "This is the central abstraction of Spark.\n",
    "* Read-only partitioned collection of records\n",
    "* Created from:\n",
    "  * Data in stable storage\n",
    "  * Transformations on other RDDs\n",
    "* Unit of parallelism in a data decomposition:\n",
    "  * Automatic parallelization of transformation, such as map, reduce, filter, etc.\n",
    "* RDDs are not data:\n",
    "  * Not materialized.  They are an abstraction.\n",
    "  * Defined by lineage. Set of transformations on a original data set.\n",
    "  \n",
    "A first example:\n",
    "\n",
    "```scala\n",
    "lines = spark.textFile(\"hdfs://...\")\n",
    "errors = lines.filter(_.startsWith(\"ERROR\"))\n",
    "errors.persist()\n",
    "\n",
    "// Return the time fields of errors mentioning \n",
    "// HDFS as an array (assuming time is field #3 \n",
    "// in a tab separated format\n",
    "errors.filter(_contains(\"HDFS\"))\n",
    "      .map(_.split('\\t')(3))\n",
    "      .collect()\n",
    "```\n",
    "\n",
    "* RDDS in this computation\n",
    "  * `lines` is RDD backed by HDFS.\n",
    "  * `errors` is derived from filter.\n",
    "  * next two are implicit (not named variables)\n",
    "* `persist` indicates to store something in memory for reuse\n",
    "* `collect` materializes computation to HDFS\n",
    "\n",
    "Associate with each Spark computation is a lineago of RDDS.\n",
    "\n",
    "<img src=\"./images/spark_lineage.png\" width=384 />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Real Example\n",
    "\n",
    "Monte Carlo approximation of $\\Pi$ based on determining whether points are inside or outside a radius 1 circle of area $\\Pi r^2$ inside a square of area $4r^2$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=App Name, master=local) created by __init__ at <ipython-input-1-e20b88521d3b>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e20b88521d3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"App Name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=App Name, master=local) created by __init__ at <ipython-input-1-e20b88521d3b>:2 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "\n",
    "import random\n",
    "num_samples = 1000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistical Regression in Spark\n",
    "\n",
    "```scala\n",
    "val points = spark.textFile(...)\n",
    "                  .map(parsePoint).persist()\n",
    "var = w  // random initial vector\n",
    "for (i <- 1 to ITERATIONS) {\n",
    "    val gradient = points.map{ p => \n",
    "          p.x * (1/(1+exp(-p.y*(w dot p.x)-1)*p.y\n",
    "    }.reduce((a+b) => a+b) \n",
    "    w -= gradient\n",
    "}\n",
    "```\n",
    "\n",
    "* Features:\n",
    "  * Scala closures, functions with free variables\n",
    "  * `points` is a read-only RDD reused in each iteration\n",
    "  * Only w (a scalar) gets updated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing Memory in Spark\n",
    "\n",
    "* `persist()` indicates desire to reuse an RDD, encourages Spark to keep it in memory\n",
    "* Spark breaks data up into: \n",
    "  * RDD: the representation of a logical data set\n",
    "  * sequence: a physical, materialized data set\n",
    "* In Spark-land, RDDs and sequences are differentiated by the concepts of \n",
    "  * Transformations: RDD->RDD\n",
    "  * Actions: RDD->sequence/data\n",
    "* RDDs define a pipeline of computations from data set (HDFS) to sequence/data\n",
    "* RDDs evaluated lazily as needed to build a sequence\n",
    "  * A sequence computation pulls data through the Spark pipeline\n",
    "* Parallelized constructs in Spark\n",
    "  * Transformations are lazy whereas actions launch computation\n",
    "  \n",
    "<img src=\"./images/spark_ops.png\" width=768 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map/Reduce in Spark\n",
    "\n",
    "The following steps compute a Map/Reduce\n",
    "```\n",
    "spark.textfile(...).flatMap(...).reduceByKey(...).save()\n",
    "```\n",
    "* Doesn’t use RDD pipelining.  \n",
    "* `flatMap` produces a sequence.\n",
    "* Doesn’t use memory abstraction\n",
    "\n",
    "#### Many Maps\n",
    "\n",
    "* `map()` is one-to-one consistent w/ scala semantics\n",
    "* `flatMap` is many-to-one like Map in M/R\n",
    "* `mapValues` does not transform key (important for partitioning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineage and Recovery\n",
    "\n",
    "* Spark in the presence of failures:\n",
    "  * Identify partitions of data (in an RDD) that have failed\n",
    "  * Recompute the failed partitions using lineage\n",
    "  * Parallelize recomputation (using Spark)\n",
    "  * Easy because all RDDs are immutable\n",
    "* Does not require checkpoint/restart or rollback\n",
    "  * Checkpoint = save required memory (application state) to persistent storage sufficient to restart the computation\n",
    "  * Restart = restart computation from a checkpoint\n",
    "  * Rollback = Undo changes made to memory associated with computations that have failed or did not complete\n",
    "  * All concepts related to managing a writeable memory related to computational progress in the presence of failures.\n",
    "  \n",
    "#### Spark Checkpointing\n",
    "\n",
    "* Spark supports checkpoints\n",
    "  * performance optimization\n",
    "  * make an RDD persistent to limit recovery work\n",
    "* It is desirable to checkpoint when:\n",
    "  * Lineages become long (in dependencies)\n",
    "  * Dependencies become wide\n",
    "* Spark’s default is to use the initial data load as the only checkpoint and restart from that checkpoint\n",
    "\n",
    "**PageRank** Checkpoint Example. Scala code and the resulting lineage.\n",
    "```scala\n",
    "val links = spark.textFile(...).map(...).persist()\n",
    "var ranks = // RDD of (URL, rank) pairs\n",
    "for (i <- 1 to ITERATIONS) {\n",
    "    // Build an RDD of (TargetURL, float) pairs\n",
    "    // with contributions sent by each page\n",
    "    val contribs = links.join(ranks).flatMap {\n",
    "        (url, (links, rank)) => \n",
    "            links.map(dest => dest, ranks/link.size))\n",
    "    }\n",
    "    // Sum contributions by URL and get new ranks\n",
    "    ranks = contribs.reduceByKey((x,y) => x+y)\n",
    "        .mapValues(sum => a/N + (1-a)*sum)\n",
    "}\n",
    "```\n",
    "<img src=\"images/sparkpr.png\" width=384 />\n",
    "\n",
    "Recovering the computation fo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
